

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>deepspeed.pt package &mdash; DeepSpeed 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="deepspeed package" href="deepspeed.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> DeepSpeed
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">deepspeed</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="deepspeed.html">deepspeed package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="deepspeed.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">deepspeed.pt package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="deepspeed.html#module-deepspeed">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">DeepSpeed</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">deepspeed</a> &raquo;</li>
        
          <li><a href="deepspeed.html">deepspeed package</a> &raquo;</li>
        
      <li>deepspeed.pt package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/microsoft/DeepSpeed/blob/master/code-docs/deepspeed.pt.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deepspeed-pt-package">
<h1>deepspeed.pt package<a class="headerlink" href="#deepspeed-pt-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-deepspeed.pt.deepspeed_config">
<span id="deepspeed-pt-deepspeed-config-module"></span><h2>deepspeed.pt.deepspeed_config module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_config" title="Permalink to this headline">¶</a></h2>
<p>Copyright (c) Microsoft Corporation
Licensed under the MIT license.</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_config.DeepSpeedConfig">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">DeepSpeedConfig</code><span class="sig-paren">(</span><em class="sig-param">json_file</em>, <em class="sig-param">mpu=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.DeepSpeedConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_config.DeepSpeedConfig.print">
<code class="sig-name descname">print</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.DeepSpeedConfig.print" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_allgather_size">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_allgather_size</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_allgather_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_allreduce_always_fp32">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_allreduce_always_fp32</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_allreduce_always_fp32" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_disable_allgather">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_disable_allgather</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_disable_allgather" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_dump_state">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_dump_state</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_dump_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_dynamic_loss_scale_args">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_dynamic_loss_scale_args</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_dynamic_loss_scale_args" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_fp16_enabled">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_fp16_enabled</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_fp16_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_gradient_accumulation_steps">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_gradient_accumulation_steps</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_gradient_accumulation_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_gradient_clipping">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_gradient_clipping</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_initial_dynamic_scale">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_initial_dynamic_scale</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_initial_dynamic_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_loss_scale">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_loss_scale</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_optimizer_gradient_clipping">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_optimizer_gradient_clipping</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_optimizer_gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_optimizer_legacy_fusion">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_optimizer_legacy_fusion</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_optimizer_legacy_fusion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_optimizer_name">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_optimizer_name</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_optimizer_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_optimizer_params">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_optimizer_params</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_prescale_gradients">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_prescale_gradients</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_prescale_gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_scalar_param">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_scalar_param</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em>, <em class="sig-param">param_name</em>, <em class="sig-param">param_default_value</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_scalar_param" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_scheduler_name">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_scheduler_name</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_scheduler_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_scheduler_params">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_scheduler_params</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_sparse_gradients_enabled">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_sparse_gradients_enabled</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_sparse_gradients_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_steps_per_print">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_steps_per_print</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_steps_per_print" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_tensorboard_enabled">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_tensorboard_enabled</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_tensorboard_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_tensorboard_job_name">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_tensorboard_job_name</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_tensorboard_job_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_tensorboard_output_path">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_tensorboard_output_path</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_tensorboard_output_path" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_train_batch_size">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_train_batch_size</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_train_batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_train_micro_batch_size_per_gpu">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_train_micro_batch_size_per_gpu</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_train_micro_batch_size_per_gpu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_wall_clock_breakdown">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_wall_clock_breakdown</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_wall_clock_breakdown" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_config.get_zero_enabled">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_config.</code><code class="sig-name descname">get_zero_enabled</code><span class="sig-paren">(</span><em class="sig-param">param_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_config.get_zero_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_constants">
<span id="deepspeed-pt-deepspeed-constants-module"></span><h2>deepspeed.pt.deepspeed_constants module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_constants" title="Permalink to this headline">¶</a></h2>
<p>Copyright (c) Microsoft Corporation
Licensed under the MIT license.</p>
</div>
<div class="section" id="module-deepspeed.pt.deepspeed_csr_tensor">
<span id="deepspeed-pt-deepspeed-csr-tensor-module"></span><h2>deepspeed.pt.deepspeed_csr_tensor module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_csr_tensor" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2020 The Microsoft DeepSpeed Team</p>
<p>Implementation of a compressed sparse row (CSR) tensor. Similar in
functionality to TensorFlow’s IndexedSlices implementation.</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_csr_tensor.CSRTensor">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_csr_tensor.</code><code class="sig-name descname">CSRTensor</code><span class="sig-paren">(</span><em class="sig-param">dense_tensor=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_csr_tensor.CSRTensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Compressed Sparse Row (CSR) Tensor</p>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_csr_tensor.CSRTensor.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">b</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_csr_tensor.CSRTensor.add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_csr_tensor.CSRTensor.sparse_size">
<code class="sig-name descname">sparse_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_csr_tensor.CSRTensor.sparse_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_csr_tensor.CSRTensor.to_dense">
<code class="sig-name descname">to_dense</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_csr_tensor.CSRTensor.to_dense" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_csr_tensor.CSRTensor.type">
<em class="property">static </em><code class="sig-name descname">type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_csr_tensor.CSRTensor.type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_dataloader">
<span id="deepspeed-pt-deepspeed-dataloader-module"></span><h2>deepspeed.pt.deepspeed_dataloader module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_dataloader" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_dataloader.DeepSpeedDataLoader">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_dataloader.</code><code class="sig-name descname">DeepSpeedDataLoader</code><span class="sig-paren">(</span><em class="sig-param">dataset</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">pin_memory</em>, <em class="sig-param">local_rank</em>, <em class="sig-param">tput_timer</em>, <em class="sig-param">collate_fn=None</em>, <em class="sig-param">num_local_io_workers=None</em>, <em class="sig-param">data_sampler=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_dataloader.DeepSpeedDataLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
</dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_fused_lamb">
<span id="deepspeed-pt-deepspeed-fused-lamb-module"></span><h2>deepspeed.pt.deepspeed_fused_lamb module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_fused_lamb" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<p>Copyright NVIDIA/apex
This file is adapted from NVIDIA/apex/optimizer/fused_adam and implements the LAMB optimizer</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_fused_lamb.FusedLamb">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_fused_lamb.</code><code class="sig-name descname">FusedLamb</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">bias_correction=True</em>, <em class="sig-param">betas=(0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">eps_inside_sqrt=False</em>, <em class="sig-param">weight_decay=0.0</em>, <em class="sig-param">max_grad_norm=0.0</em>, <em class="sig-param">max_coeff=10.0</em>, <em class="sig-param">min_coeff=0.01</em>, <em class="sig-param">amsgrad=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_fused_lamb.FusedLamb" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>Implements LAMB algorithm. Currently GPU-only.  Requires DeepSpeed adapted Apex to be installed via
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span> <span class="pre">--cuda_ext</span> <span class="pre">--cpp_ext</span></code>.</p>
<p>For usage example please see, TODO DeepSpeed Tutorial</p>
<p>It has been proposed in <a href="#id1"><span class="problematic" id="id2">`</span></a>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.
<a class="reference external" href="https://arxiv.org/abs/1904.00962">https://arxiv.org/abs/1904.00962</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups.</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – learning rate. (default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – coefficients used for computing
running averages of gradient and its square. (default: (0.9, 0.999))</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability. (default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
<li><p><strong>max_coeff</strong> (<em>float</em><em>, </em><em>optional</em>) – maximum value of the lamb coefficient (default: 10.0)</p></li>
<li><p><strong>min_coeff</strong> (<em>float</em><em>, </em><em>optional</em>) – minimum value of the lamb coefficient (default: 0.01)</p></li>
<li><p><strong>amsgrad</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether to use the AMSGrad variant of this
algorithm from the paper <a class="reference external" href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a>
(default: False) NOT SUPPORTED in FusedAdam!</p></li>
<li><p><strong>eps_inside_sqrt</strong> (<em>boolean</em><em>, </em><em>optional</em>) – in the ‘update parameters’ step,
adds eps to the bias-corrected second moment estimate before
evaluating square root instead of adding it to the square root of
second moment estimate as in the original paper. (default: False)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_fused_lamb.FusedLamb.get_lamb_coeffs">
<code class="sig-name descname">get_lamb_coeffs</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_fused_lamb.FusedLamb.get_lamb_coeffs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_fused_lamb.FusedLamb.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">closure=None</em>, <em class="sig-param">grads=None</em>, <em class="sig-param">output_params=None</em>, <em class="sig-param">scale=1.0</em>, <em class="sig-param">grad_norms=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_fused_lamb.FusedLamb.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p></li>
<li><p><strong>grads</strong> (<em>list of tensors</em><em>, </em><em>optional</em>) – weight gradient to use for the
optimizer update. If gradients have type torch.half, parameters
are expected to be in type torch.float. (default: None)</p></li>
<li><p><strong>params</strong> (<em>output</em>) – A reduced precision copy
of the updated weights written out in addition to the regular
updated weights. Have to be of same type as gradients. (default: None)</p></li>
<li><p><strong>scale</strong> (<em>float</em><em>, </em><em>optional</em>) – factor to divide gradient tensor values
by before applying to weights. (default: 1)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_launch">
<span id="deepspeed-pt-deepspeed-launch-module"></span><h2>deepspeed.pt.deepspeed_launch module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_launch" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2020 The Microsoft DeepSpeed Team: <a class="reference external" href="mailto:deepspeed&#37;&#52;&#48;microsoft&#46;com">deepspeed<span>&#64;</span>microsoft<span>&#46;</span>com</a></p>
<dl class="function">
<dt id="deepspeed.pt.deepspeed_launch.main">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_launch.</code><code class="sig-name descname">main</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_launch.main" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_launch.parse_args">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_launch.</code><code class="sig-name descname">parse_args</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_launch.parse_args" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_light">
<span id="deepspeed-pt-deepspeed-light-module"></span><h2>deepspeed.pt.deepspeed_light module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_light" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_light.</code><code class="sig-name descname">DeepSpeedLight</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">model</em>, <em class="sig-param">optimizer=None</em>, <em class="sig-param">model_parameters=None</em>, <em class="sig-param">training_data=None</em>, <em class="sig-param">lr_scheduler=None</em>, <em class="sig-param">mpu=None</em>, <em class="sig-param">dist_init_required=None</em>, <em class="sig-param">collate_fn=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>DeepSpeed engine for training.</p>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.all_gather_scalar">
<code class="sig-name descname">all_gather_scalar</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.all_gather_scalar" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.allgather_size">
<code class="sig-name descname">allgather_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.allgather_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_always_fp32">
<code class="sig-name descname">allreduce_always_fp32</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_always_fp32" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_and_copy">
<code class="sig-name descname">allreduce_and_copy</code><span class="sig-paren">(</span><em class="sig-param">small_bucket</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_and_copy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_bucket">
<code class="sig-name descname">allreduce_bucket</code><span class="sig-paren">(</span><em class="sig-param">bucket</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_bucket" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_gradients">
<code class="sig-name descname">allreduce_gradients</code><span class="sig-paren">(</span><em class="sig-param">bucket_size=500000000</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_no_retain">
<code class="sig-name descname">allreduce_no_retain</code><span class="sig-paren">(</span><em class="sig-param">bucket</em>, <em class="sig-param">numel_per_bucket=500000000</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.allreduce_no_retain" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">allreduce_gradients=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute backward pass on the loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> – Torch tensor on which to execute backward propagation</p></li>
<li><p><strong>allreduce_gradients</strong> – If this is False, then gradient averaging will be skipped. Default is True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.buffered_allreduce_fallback">
<code class="sig-name descname">buffered_allreduce_fallback</code><span class="sig-paren">(</span><em class="sig-param">grads=None</em>, <em class="sig-param">elements_per_buffer=500000000</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.buffered_allreduce_fallback" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.csr_all_gather">
<code class="sig-name descname">csr_all_gather</code><span class="sig-paren">(</span><em class="sig-param">value</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.csr_all_gather" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.csr_allreduce">
<code class="sig-name descname">csr_allreduce</code><span class="sig-paren">(</span><em class="sig-param">csr</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.csr_allreduce" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.csr_allreduce_bucket">
<code class="sig-name descname">csr_allreduce_bucket</code><span class="sig-paren">(</span><em class="sig-param">bucket</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.csr_allreduce_bucket" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.csr_allreduce_no_retain">
<code class="sig-name descname">csr_allreduce_no_retain</code><span class="sig-paren">(</span><em class="sig-param">bucket</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.csr_allreduce_no_retain" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.deepspeed_io">
<code class="sig-name descname">deepspeed_io</code><span class="sig-paren">(</span><em class="sig-param">dataset</em>, <em class="sig-param">batch_size=None</em>, <em class="sig-param">route='train'</em>, <em class="sig-param">pin_memory=True</em>, <em class="sig-param">data_sampler=None</em>, <em class="sig-param">collate_fn=None</em>, <em class="sig-param">num_local_io_workers=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.deepspeed_io" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.disable_allgather">
<code class="sig-name descname">disable_allgather</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.disable_allgather" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.dump_state">
<code class="sig-name descname">dump_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.dump_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.dynamic_loss_scale">
<code class="sig-name descname">dynamic_loss_scale</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.dynamic_loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.dynamic_loss_scale_args">
<code class="sig-name descname">dynamic_loss_scale_args</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.dynamic_loss_scale_args" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.eval" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute forward propagation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*inputs</strong> – Variable length input list</p></li>
<li><p><strong>**kwargs</strong> – variable length keyword arguments</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.fp16_enabled">
<code class="sig-name descname">fp16_enabled</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.fp16_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.get_lr">
<code class="sig-name descname">get_lr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.get_lr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.get_mom">
<code class="sig-name descname">get_mom</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.get_mom" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.get_summary_writer">
<code class="sig-name descname">get_summary_writer</code><span class="sig-paren">(</span><em class="sig-param">name='DeepSpeedJobName'</em>, <em class="sig-param">base='/home/shsmit/tensorboard'</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.get_summary_writer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.gradient_accumulation_steps">
<code class="sig-name descname">gradient_accumulation_steps</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.gradient_accumulation_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.gradient_clipping">
<code class="sig-name descname">gradient_clipping</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.initial_dynamic_scale">
<code class="sig-name descname">initial_dynamic_scale</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.initial_dynamic_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.is_gradient_accumulation_boundary">
<code class="sig-name descname">is_gradient_accumulation_boundary</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.is_gradient_accumulation_boundary" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.load_checkpoint">
<code class="sig-name descname">load_checkpoint</code><span class="sig-paren">(</span><em class="sig-param">load_dir</em>, <em class="sig-param">tag</em>, <em class="sig-param">load_optimizer_states=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Load training checkpoint</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>load_dir</strong> – Required. Directory to load the checkpoint from</p></li>
<li><p><strong>tag</strong> – Required. Checkpoint tag used as a unique identifier for the checkpoint. Ex. Global Step.</p></li>
<li><p><strong>load_optimizer_states</strong> – Optional. Boolean to load the training optimizer states from Checkpoint. Ex. ADAM’s momentum and variance</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Path of the loaded checkpoint. None if loading the checkpoint failed
client_state: State dictionary used for loading required training states in the client code.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>load_path</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.load_module_state_dict">
<code class="sig-name descname">load_module_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em>, <em class="sig-param">strict=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.load_module_state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.loss_scale">
<code class="sig-name descname">loss_scale</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.module_state_dict">
<code class="sig-name descname">module_state_dict</code><span class="sig-paren">(</span><em class="sig-param">destination=None</em>, <em class="sig-param">prefix=''</em>, <em class="sig-param">keep_vars=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.module_state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.optimizer_legacy_fusion">
<code class="sig-name descname">optimizer_legacy_fusion</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.optimizer_legacy_fusion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.optimizer_name">
<code class="sig-name descname">optimizer_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.optimizer_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.optimizer_params">
<code class="sig-name descname">optimizer_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.optimizer_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.postscale_gradients">
<code class="sig-name descname">postscale_gradients</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.postscale_gradients" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.save_checkpoint">
<code class="sig-name descname">save_checkpoint</code><span class="sig-paren">(</span><em class="sig-param">save_dir</em>, <em class="sig-param">tag</em>, <em class="sig-param">client_state={}</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Save training checkpoint</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>save_dir</strong> – Required. Directory for saving the checkpoint</p></li>
<li><p><strong>tag</strong> – Required. Checkpoint tag used as a unique identifier for the checkpoint. Ex. Global Step.</p></li>
<li><p><strong>client_state</strong> – Optional. State dictionary used for saving required training states in the client code.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.scheduler_name">
<code class="sig-name descname">scheduler_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.scheduler_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.scheduler_params">
<code class="sig-name descname">scheduler_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.scheduler_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.sparse_gradients_enabled">
<code class="sig-name descname">sparse_gradients_enabled</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.sparse_gradients_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Execute the weight update step after forward and backward propagation on effective_train_batch</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.steps_per_print">
<code class="sig-name descname">steps_per_print</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.steps_per_print" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.tensorboard_enabled">
<code class="sig-name descname">tensorboard_enabled</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.tensorboard_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.tensorboard_job_name">
<code class="sig-name descname">tensorboard_job_name</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.tensorboard_job_name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.tensorboard_output_path">
<code class="sig-name descname">tensorboard_output_path</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.tensorboard_output_path" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.train" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.train_batch_size">
<code class="sig-name descname">train_batch_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.train_batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.train_micro_batch_size_per_gpu">
<code class="sig-name descname">train_micro_batch_size_per_gpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.train_micro_batch_size_per_gpu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.wall_clock_breakdown">
<code class="sig-name descname">wall_clock_breakdown</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.wall_clock_breakdown" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_light.DeepSpeedLight.zero_optimization">
<code class="sig-name descname">zero_optimization</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.DeepSpeedLight.zero_optimization" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_light.print_configuration">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_light.</code><code class="sig-name descname">print_configuration</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">name</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.print_configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_light.split_half_float_double_csr">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_light.</code><code class="sig-name descname">split_half_float_double_csr</code><span class="sig-paren">(</span><em class="sig-param">tensors</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_light.split_half_float_double_csr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_lr_schedules">
<span id="deepspeed-pt-deepspeed-lr-schedules-module"></span><h2>deepspeed.pt.deepspeed_lr_schedules module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_lr_schedules" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<p>Implementation of learning rate schedules.</p>
<p>Taken and modified from PyTorch v1.0.1 source
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/v1.1.0/torch/optim/lr_scheduler.py">https://github.com/pytorch/pytorch/blob/v1.1.0/torch/optim/lr_scheduler.py</a></p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">LRRangeTest</code><span class="sig-paren">(</span><em class="sig-param">optimizer: torch.optim.optimizer.Optimizer</em>, <em class="sig-param">lr_range_test_min_lr: float = 0.001</em>, <em class="sig-param">lr_range_test_step_size: int = 2000</em>, <em class="sig-param">lr_range_test_step_rate: float = 1.0</em>, <em class="sig-param">lr_range_test_staircase: bool = False</em>, <em class="sig-param">last_batch_iteration: int = -1</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Sets the learning rate of each parameter group according to
learning rate range test (LRRT) policy. The policy increases learning
rate starting from a base value with a constant frequency, as detailed in
the paper <a href="#id3"><span class="problematic" id="id4">`A disciplined approach to neural network hyper-parameters: Part1`_</span></a>.</p>
<p>LRRT policy is used for finding maximum LR that trains a model without divergence, and can be used to
configure the LR boundaries for Cylic LR schedules.</p>
<p>LRRT changes the learning rate after every batch.
<cite>step</cite> should be called after a batch has been used for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<em>Optimizer</em>) – Wrapped optimizer.</p></li>
<li><p><strong>lr_range_test_min_lr</strong> (<em>float</em><em> or </em><em>list</em>) – Initial learning rate which is the
lower boundary in the range test for each parameter group.</p></li>
<li><p><strong>lr_range_test_step_size</strong> (<em>int</em>) – Interval of training steps to increase learning rate. Default: 2000</p></li>
<li><p><strong>lr_range_test_step_rate</strong> (<em>float</em>) – Scaling rate for range test. Default: 1.0</p></li>
<li><p><strong>lr_range_test_staircase</strong> (<em>bool</em>) – Scale in staircase fashion, rather than continous. Default: False.</p></li>
<li><p><strong>last_batch_iteration</strong> (<em>int</em>) – The index of the last batch. This parameter is used when
resuming a training job. Since <cite>step()</cite> should be invoked after each
batch instead of after each epoch, this number represents the total
number of <em>batches</em> computed, not the total number of epochs computed.
When last_batch_iteration=-1, the schedule is started from the beginning.
Default: -1</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">LRRangeTest</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">train_batch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>_A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay:
<a class="reference external" href="https://arxiv.org/abs/1803.09820">https://arxiv.org/abs/1803.09820</a></p>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.get_lr">
<code class="sig-name descname">get_lr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.get_lr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">sd</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">batch_iteration=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepspeed.pt.deepspeed_lr_schedules.OneCycle">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">OneCycle</code><span class="sig-paren">(</span><em class="sig-param">optimizer</em>, <em class="sig-param">cycle_min_lr</em>, <em class="sig-param">cycle_max_lr</em>, <em class="sig-param">decay_lr_rate=0.0</em>, <em class="sig-param">cycle_first_step_size=2000</em>, <em class="sig-param">cycle_second_step_size=None</em>, <em class="sig-param">cycle_first_stair_count=0</em>, <em class="sig-param">cycle_second_stair_count=None</em>, <em class="sig-param">decay_step_size=0</em>, <em class="sig-param">cycle_momentum=True</em>, <em class="sig-param">cycle_min_mom=0.8</em>, <em class="sig-param">cycle_max_mom=0.9</em>, <em class="sig-param">decay_mom_rate=0.0</em>, <em class="sig-param">last_batch_iteration=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Sets the learning rate of each parameter group according to
1Cycle learning rate policy (1CLR). 1CLR is a variation of the
Cyclical Learning Rate (CLR) policy that involves one cycle followed by
decay. The policy simultaneously cycles the learning rate (and momentum)
between two boundaries with a constant frequency, as detailed in
the paper <a class="reference external" href="Part1--learningrate,batchsize,momentum,andweightdecay:https://arxiv.org/abs/1803.09820">A disciplined approach to neural network hyper-parameters</a>.</p>
<p>1CLR policy changes the learning rate after every batch.
<cite>step</cite> should be called after a batch has been used for training.</p>
<p>This implementation was adapted from the github repo: <a href="#id5"><span class="problematic" id="id6">`pytorch/pytorch`_</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<em>Optimizer</em>) – Wrapped optimizer.</p></li>
<li><p><strong>cycle_min_lr</strong> (<em>float</em><em> or </em><em>list</em>) – Initial learning rate which is the
lower boundary in the cycle for each parameter group.</p></li>
<li><p><strong>cycle_max_lr</strong> (<em>float</em><em> or </em><em>list</em>) – Upper learning rate boundaries in the cycle
for each parameter group. Functionally,
it defines the cycle amplitude (cycle_max_lr - cycle_min_lr).
The lr at any cycle is the sum of cycle_min_lr
and some scaling of the amplitude; therefore
cycle_max_lr may not actually be reached depending on
scaling function.</p></li>
<li><p><strong>decay_lr_rate</strong> (<em>float</em>) – Decay rate for learning rate. Default: 0.</p></li>
<li><p><strong>cycle_first_step_size</strong> (<em>int</em>) – Number of training iterations in the
increasing half of a cycle. Default: 2000</p></li>
<li><p><strong>cycle_second_step_size</strong> (<em>int</em>) – Number of training iterations in the
decreasing half of a cycle. If cycle_second_step_size is None,
it is set to cycle_first_step_size. Default: None</p></li>
<li><p><strong>cycle_first_stair_count</strong> (<em>int</em>) – Number of stairs in first half of cycle phase. This means</p></li>
<li><p><strong>are changed in staircase fashion. Default 0</strong><strong>, </strong><strong>means staircase disabled.</strong> (<em>lr/mom</em>) – </p></li>
<li><p><strong>cycle_second_stair_count</strong> (<em>int</em>) – Number of stairs in second half of cycle phase. This means</p></li>
<li><p><strong>are changed in staircase fashion. Default 0</strong><strong>, </strong><strong>means staircase disabled.</strong> – </p></li>
<li><p><strong>decay_step_size</strong> (<em>int</em>) – Intervals for applying decay in decay phase. Default: 0, means no decay.</p></li>
<li><p><strong>cycle_momentum</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, momentum is cycled inversely
to learning rate between ‘cycle_min_mom’ and ‘cycle_max_mom’.
Default: True</p></li>
<li><p><strong>cycle_min_mom</strong> (<em>float</em><em> or </em><em>list</em>) – Initial momentum which is the
lower boundary in the cycle for each parameter group.
Default: 0.8</p></li>
<li><p><strong>cycle_max_mom</strong> (<em>float</em><em> or </em><em>list</em>) – Upper momentum boundaries in the cycle
for each parameter group. Functionally,
it defines the cycle amplitude (cycle_max_mom - cycle_min_mom).
The momentum at any cycle is the difference of cycle_max_mom
and some scaling of the amplitude; therefore
cycle_min_mom may not actually be reached depending on
scaling function. Default: 0.9</p></li>
<li><p><strong>decay_mom_rate</strong> (<em>float</em>) – Decay rate for momentum. Default: 0.</p></li>
<li><p><strong>last_batch_iteration</strong> (<em>int</em>) – The index of the last batch. This parameter is used when
resuming a training job. Since <cite>step()</cite> should be invoked after each
batch instead of after each epoch, this number represents the total
number of <em>batches</em> computed, not the total number of epochs computed.
When last_batch_iteration=-1, the schedule is started from the beginning.
Default: -1</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">OneCycle</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">train_batch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.get_lr">
<code class="sig-name descname">get_lr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.get_lr" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the learning rate at batch index. This function treats
<cite>self.last_batch_iteration</cite> as the last batch index.</p>
<p>If <cite>self.cycle_momentum</cite> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, this function has a side effect of
updating the optimizer’s momentum.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">sd</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">batch_iteration=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">WarmupLR</code><span class="sig-paren">(</span><em class="sig-param">optimizer: torch.optim.optimizer.Optimizer</em>, <em class="sig-param">warmup_min_lr: float = 0.0</em>, <em class="sig-param">warmup_max_lr: float = 0.001</em>, <em class="sig-param">warmup_num_steps: int = 1000</em>, <em class="sig-param">last_batch_iteration: int = -1</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Increase the learning rate of each parameter group from min lr to max lr
over warmup_num_steps steps, and then fix at max lr.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<em>Optimizer</em>) – Wrapped optimizer.</p></li>
<li><p><strong>warmup_min_lr</strong> (<em>float</em><em> or </em><em>list</em>) – minimum learning rate. Default: 0</p></li>
<li><p><strong>warmup_max_lr</strong> (<em>float</em><em> or </em><em>list</em>) – maximum learning rate. Default: 0.001</p></li>
<li><p><strong>warmup_num_steps</strong> (<em>int</em>) – number of steps to warm up from min_lr to max_lr. Default: 1000</p></li>
<li><p><strong>last_batch_iteration</strong> (<em>int</em>) – The index of the last batch. Default: -1.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">WarmupLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">train_batch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.get_lr">
<code class="sig-name descname">get_lr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.get_lr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">sd</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">last_batch_iteration=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.add_tuning_arguments">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">add_tuning_arguments</code><span class="sig-paren">(</span><em class="sig-param">parser</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.add_tuning_arguments" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.get_config_from_args">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">get_config_from_args</code><span class="sig-paren">(</span><em class="sig-param">args</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.get_config_from_args" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.get_lr_from_config">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">get_lr_from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.get_lr_from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.get_torch_optimizer">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">get_torch_optimizer</code><span class="sig-paren">(</span><em class="sig-param">optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.get_torch_optimizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.override_1cycle_params">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">override_1cycle_params</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.override_1cycle_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.override_lr_range_test_params">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">override_lr_range_test_params</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.override_lr_range_test_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.override_params">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">override_params</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.override_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.override_warmupLR_params">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">override_warmupLR_params</code><span class="sig-paren">(</span><em class="sig-param">args</em>, <em class="sig-param">params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.override_warmupLR_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_lr_schedules.parse_arguments">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_lr_schedules.</code><code class="sig-name descname">parse_arguments</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_lr_schedules.parse_arguments" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_run">
<span id="deepspeed-pt-deepspeed-run-module"></span><h2>deepspeed.pt.deepspeed_run module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_run" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2020 The Microsoft DeepSpeed Team</p>
<dl class="function">
<dt id="deepspeed.pt.deepspeed_run.encode_world_info">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_run.</code><code class="sig-name descname">encode_world_info</code><span class="sig-paren">(</span><em class="sig-param">world_info</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_run.encode_world_info" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_run.fetch_hostfile">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_run.</code><code class="sig-name descname">fetch_hostfile</code><span class="sig-paren">(</span><em class="sig-param">hostfile_path</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_run.fetch_hostfile" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_run.main">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_run.</code><code class="sig-name descname">main</code><span class="sig-paren">(</span><em class="sig-param">args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_run.main" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_run.parse_args">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_run.</code><code class="sig-name descname">parse_args</code><span class="sig-paren">(</span><em class="sig-param">args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_run.parse_args" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_run.parse_inclusion_exclusion">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_run.</code><code class="sig-name descname">parse_inclusion_exclusion</code><span class="sig-paren">(</span><em class="sig-param">resource_pool</em>, <em class="sig-param">inclusion</em>, <em class="sig-param">exclusion</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_run.parse_inclusion_exclusion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_run.parse_resource_filter">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_run.</code><code class="sig-name descname">parse_resource_filter</code><span class="sig-paren">(</span><em class="sig-param">host_info</em>, <em class="sig-param">include_str=''</em>, <em class="sig-param">exclude_str=''</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_run.parse_resource_filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse an inclusion or exclusion string and filter a hostfile dictionary.</p>
<dl class="simple">
<dt>String format is NODE_SPEC[&#64;NODE_SPEC …], where</dt><dd><p>NODE_SPEC = NAME[:SLOT[,SLOT …]].</p>
</dd>
</dl>
<p>If :SLOT is omitted, include/exclude all slots on that host.</p>
<p class="rubric">Examples</p>
<dl class="simple">
<dt>include_str=”<a class="reference external" href="mailto:worker-0&#37;&#52;&#48;worker-1">worker-0<span>&#64;</span>worker-1</a>:0,2” will use all slots on worker-0 and</dt><dd><p>slots [0, 2] on worker-1.</p>
</dd>
<dt>exclude_str=”worker-1:0” will use all available resources except</dt><dd><p>slot 0 on worker-1.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_timer">
<span id="deepspeed-pt-deepspeed-timer-module"></span><h2>deepspeed.pt.deepspeed_timer module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_timer" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_timer.</code><code class="sig-name descname">SynchronizedWallClockTimer</code><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Group of timers. Borrowed from Nvidia Megatron code</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer">
<em class="property">class </em><code class="sig-name descname">Timer</code><span class="sig-paren">(</span><em class="sig-param">name</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Timer.</p>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer.elapsed">
<code class="sig-name descname">elapsed</code><span class="sig-paren">(</span><em class="sig-param">reset=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer.elapsed" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the elapsed time.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset timer.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer.start">
<code class="sig-name descname">start</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer.start" title="Permalink to this definition">¶</a></dt>
<dd><p>Start the timer.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer.stop">
<code class="sig-name descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.Timer.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stop the timer.</p>
</dd></dl>

</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.log">
<code class="sig-name descname">log</code><span class="sig-paren">(</span><em class="sig-param">names</em>, <em class="sig-param">normalizer=1.0</em>, <em class="sig-param">reset=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.SynchronizedWallClockTimer.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a group of timers.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepspeed.pt.deepspeed_timer.ThroughputTimer">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_timer.</code><code class="sig-name descname">ThroughputTimer</code><span class="sig-paren">(</span><em class="sig-param">batch_size</em>, <em class="sig-param">num_workers</em>, <em class="sig-param">start_step=2</em>, <em class="sig-param">steps_per_output=50</em>, <em class="sig-param">monitor_memory=True</em>, <em class="sig-param">logging_fn=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.ThroughputTimer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.ThroughputTimer.avg_samples_per_sec">
<code class="sig-name descname">avg_samples_per_sec</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.ThroughputTimer.avg_samples_per_sec" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.ThroughputTimer.start">
<code class="sig-name descname">start</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.ThroughputTimer.start" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.ThroughputTimer.stop">
<code class="sig-name descname">stop</code><span class="sig-paren">(</span><em class="sig-param">report_speed=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.ThroughputTimer.stop" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_timer.ThroughputTimer.update_epoch_count">
<code class="sig-name descname">update_epoch_count</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.ThroughputTimer.update_epoch_count" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_timer.print_rank_0">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_timer.</code><code class="sig-name descname">print_rank_0</code><span class="sig-paren">(</span><em class="sig-param">message</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_timer.print_rank_0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_utils">
<span id="deepspeed-pt-deepspeed-utils-module"></span><h2>deepspeed.pt.deepspeed_utils module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_utils" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<p>Copyright NVIDIA/Megatron</p>
<p>Helper functions and classes from multiple sources.</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_utils.CheckOverflow">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_utils.</code><code class="sig-name descname">CheckOverflow</code><span class="sig-paren">(</span><em class="sig-param">param_groups=None</em>, <em class="sig-param">mpu=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_utils.CheckOverflow" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Checks for overflow in gradient across parallel process</p>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_utils.CheckOverflow.check">
<code class="sig-name descname">check</code><span class="sig-paren">(</span><em class="sig-param">param_groups=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_utils.CheckOverflow.check" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_utils.CheckOverflow.check_using_norm">
<code class="sig-name descname">check_using_norm</code><span class="sig-paren">(</span><em class="sig-param">norm_group</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_utils.CheckOverflow.check_using_norm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_utils.CheckOverflow.has_overflow">
<code class="sig-name descname">has_overflow</code><span class="sig-paren">(</span><em class="sig-param">params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_utils.CheckOverflow.has_overflow" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_utils.CheckOverflow.has_overflow_serial">
<code class="sig-name descname">has_overflow_serial</code><span class="sig-paren">(</span><em class="sig-param">params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_utils.CheckOverflow.has_overflow_serial" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_utils.get_grad_norm">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_utils.</code><code class="sig-name descname">get_grad_norm</code><span class="sig-paren">(</span><em class="sig-param">parameters</em>, <em class="sig-param">norm_type=2</em>, <em class="sig-param">mpu=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_utils.get_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient norm of an iterable of parameters.</p>
<p>This is adapted from <a href="#id7"><span class="problematic" id="id8">torch.nn.utils.clip_grad.clip_grad_norm_</span></a> and
added functionality to handle model parallel parameters. Note that
the gradients are modified in place. Taken from Nvidia Megatron.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<em>Iterable</em><em>[</em><em>Tensor</em><em>] or </em><em>Tensor</em>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</p></li>
<li><p><strong>max_norm</strong> (<em>float</em><em> or </em><em>int</em>) – max norm of the gradients</p></li>
<li><p><strong>norm_type</strong> (<em>float</em><em> or </em><em>int</em>) – type of the used p-norm. Can be <code class="docutils literal notranslate"><span class="pre">'inf'</span></code> for
infinity norm.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Total norm of the parameters (viewed as a single vector).</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_utils.get_weight_norm">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_utils.</code><code class="sig-name descname">get_weight_norm</code><span class="sig-paren">(</span><em class="sig-param">parameters</em>, <em class="sig-param">norm_type=2</em>, <em class="sig-param">mpu=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_utils.get_weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient norm of an iterable of parameters.</p>
<p>This is adapted from <a href="#id9"><span class="problematic" id="id10">torch.nn.utils.clip_grad.clip_grad_norm_</span></a> and
added functionality to handle model parallel parameters. Note that
the gradients are modified in place. Taken from Nvidia Megatron.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<em>Iterable</em><em>[</em><em>Tensor</em><em>] or </em><em>Tensor</em>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</p></li>
<li><p><strong>max_norm</strong> (<em>float</em><em> or </em><em>int</em>) – max norm of the gradients</p></li>
<li><p><strong>norm_type</strong> (<em>float</em><em> or </em><em>int</em>) – type of the used p-norm. Can be <code class="docutils literal notranslate"><span class="pre">'inf'</span></code> for
infinity norm.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Total norm of the parameters (viewed as a single vector).</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.deepspeed_zero_optimizer">
<span id="deepspeed-pt-deepspeed-zero-optimizer-module"></span><h2>deepspeed.pt.deepspeed_zero_optimizer module<a class="headerlink" href="#module-deepspeed.pt.deepspeed_zero_optimizer" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<p>Copyright NVIDIA/apex
This file is adapted from FP16_Optimizer in NVIDIA/apex</p>
<dl class="class">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.deepspeed_zero_optimizer.</code><code class="sig-name descname">FP16_DeepSpeedZeroOptimizer</code><span class="sig-paren">(</span><em class="sig-param">init_optimizer</em>, <em class="sig-param">static_loss_scale=1.0</em>, <em class="sig-param">dynamic_loss_scale=False</em>, <em class="sig-param">dynamic_loss_args=None</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">dp_process_group=None</em>, <em class="sig-param">partition_size=None</em>, <em class="sig-param">mpu=None</em>, <em class="sig-param">all_gather_partitions=True</em>, <em class="sig-param">allgather_size=500000000</em>, <em class="sig-param">clip_grad=0.0</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>DeepSpeedZeroOptimizer designed to reduce the memory footprint
required for training large deep learning models.</p>
<p>For more details please see ZeRO: Memory Optimization Towards Training A Trillion Parameter Models
<a class="reference external" href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a></p>
<p>For usage examples, refer to TODO: DeepSpeed V2 Tutorial</p>
<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">retain_graph=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.cur_scale">
<em class="property">property </em><code class="sig-name descname">cur_scale</code><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.cur_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.free_grad_in_param_list">
<code class="sig-name descname">free_grad_in_param_list</code><span class="sig-paren">(</span><em class="sig-param">param_list</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.free_grad_in_param_list" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.get_data_parallel_partitions">
<code class="sig-name descname">get_data_parallel_partitions</code><span class="sig-paren">(</span><em class="sig-param">tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.get_data_parallel_partitions" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.get_flat_partition">
<code class="sig-name descname">get_flat_partition</code><span class="sig-paren">(</span><em class="sig-param">tensor_list</em>, <em class="sig-param">first_offset</em>, <em class="sig-param">partition_size</em>, <em class="sig-param">dtype=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.get_flat_partition" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.get_partition_info">
<code class="sig-name descname">get_partition_info</code><span class="sig-paren">(</span><em class="sig-param">tensor_list</em>, <em class="sig-param">partition_size</em>, <em class="sig-param">partition_id</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.get_partition_info" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em>, <em class="sig-param">load_optimizer_states=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a state_dict created by an earlier call to state_dict().
If <code class="docutils literal notranslate"><span class="pre">fp16_optimizer_instance</span></code> was constructed from some <code class="docutils literal notranslate"><span class="pre">init_optimizer</span></code>,
whose parameters in turn came from <code class="docutils literal notranslate"><span class="pre">model</span></code>, it is expected that the user
will call <code class="docutils literal notranslate"><span class="pre">model.load_state_dict()</span></code> before
<code class="docutils literal notranslate"><span class="pre">fp16_optimizer_instance.load_state_dict()</span></code> is called.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">FP16_Optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">static_loss_scale</span> <span class="o">=</span> <span class="mf">128.0</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;saved.pth&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">])</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.loss_scale">
<em class="property">property </em><code class="sig-name descname">loss_scale</code><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.param_groups">
<em class="property">property </em><code class="sig-name descname">param_groups</code><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.param_groups" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.print_first_n">
<code class="sig-name descname">print_first_n</code><span class="sig-paren">(</span><em class="sig-param">caption</em>, <em class="sig-param">tensor</em>, <em class="sig-param">n=10</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.print_first_n" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.see_memory_usage">
<code class="sig-name descname">see_memory_usage</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.see_memory_usage" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.state">
<em class="property">property </em><code class="sig-name descname">state</code><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dict containing the current state of this <code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code> instance.
This dict contains attributes of <code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code>, as well as the state_dict
of the contained Pytorch optimizer.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="s2">&quot;saved.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Not supporting closure.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.unscale_and_clip_grads">
<code class="sig-name descname">unscale_and_clip_grads</code><span class="sig-paren">(</span><em class="sig-param">grad_groups_flat</em>, <em class="sig-param">norm_groups</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.unscale_and_clip_grads" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><em class="sig-param">set_grads_to_None=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.FP16_DeepSpeedZeroOptimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Zero FP16 parameter grads.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepspeed.pt.deepspeed_zero_optimizer.flatten_dense_tensors_aligned">
<code class="sig-prename descclassname">deepspeed.pt.deepspeed_zero_optimizer.</code><code class="sig-name descname">flatten_dense_tensors_aligned</code><span class="sig-paren">(</span><em class="sig-param">tensor_list</em>, <em class="sig-param">alignment</em>, <em class="sig-param">pg</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.deepspeed_zero_optimizer.flatten_dense_tensors_aligned" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.fp16_optimizer">
<span id="deepspeed-pt-fp16-optimizer-module"></span><h2>deepspeed.pt.fp16_optimizer module<a class="headerlink" href="#module-deepspeed.pt.fp16_optimizer" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<p>Copyright NVIDIA/apex
This file is adapted from FP16_Optimizer in NVIDIA/apex</p>
<dl class="class">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.fp16_optimizer.</code><code class="sig-name descname">FP16_Optimizer</code><span class="sig-paren">(</span><em class="sig-param">init_optimizer</em>, <em class="sig-param">static_loss_scale=1.0</em>, <em class="sig-param">dynamic_loss_scale=False</em>, <em class="sig-param">initial_dynamic_scale=4294967296</em>, <em class="sig-param">dynamic_loss_args=None</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">mpu=None</em>, <em class="sig-param">clip_grad=0.0</em>, <em class="sig-param">fused_adam_legacy=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>FP16 Optimizer for training fp16 models. Handles loss scaling.</p>
<p>For usage example please see, TODO:  DeepSpeed V2 Tutorial</p>
<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.backward" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.backward" title="deepspeed.pt.fp16_optimizer.FP16_Optimizer.backward"><code class="xref py py-attr docutils literal notranslate"><span class="pre">backward</span></code></a> performs the following steps:</p>
<ol class="arabic simple">
<li><p>fp32_loss = loss.float()</p></li>
<li><p>scaled_loss = fp32_loss*loss_scale</p></li>
<li><p>scaled_loss.backward(), which accumulates scaled gradients into the <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes of the model’s fp16 leaves</p></li>
</ol>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em>, <em class="sig-param">load_optimizer_states=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a state_dict created by an earlier call to state_dict().
If <code class="docutils literal notranslate"><span class="pre">fp16_optimizer_instance</span></code> was constructed from some <code class="docutils literal notranslate"><span class="pre">init_optimizer</span></code>,
whose parameters in turn came from <code class="docutils literal notranslate"><span class="pre">model</span></code>, it is expected that the user
will call <code class="docutils literal notranslate"><span class="pre">model.load_state_dict()</span></code> before
<code class="docutils literal notranslate"><span class="pre">fp16_optimizer_instance.load_state_dict()</span></code> is called.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">FP16_Optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">static_loss_scale</span> <span class="o">=</span> <span class="mf">128.0</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;saved.pth&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">])</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.param_groups">
<em class="property">property </em><code class="sig-name descname">param_groups</code><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.param_groups" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.state">
<em class="property">property </em><code class="sig-name descname">state</code><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dict containing the current state of this <a class="reference internal" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer" title="deepspeed.pt.fp16_optimizer.FP16_Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code></a> instance.
This dict contains attributes of <a class="reference internal" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer" title="deepspeed.pt.fp16_optimizer.FP16_Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code></a>, as well as the state_dict
of the contained Pytorch optimizer.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="s2">&quot;saved.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Not supporting closure.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.step_fused_adam">
<code class="sig-name descname">step_fused_adam</code><span class="sig-paren">(</span><em class="sig-param">closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.step_fused_adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Not supporting closure.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.unscale_and_clip_grads">
<code class="sig-name descname">unscale_and_clip_grads</code><span class="sig-paren">(</span><em class="sig-param">grad_groups_flat</em>, <em class="sig-param">norm_groups</em>, <em class="sig-param">apply_scale=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.unscale_and_clip_grads" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_optimizer.FP16_Optimizer.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><em class="sig-param">set_grads_to_None=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_optimizer.FP16_Optimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Zero FP16 parameter grads.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.fp16_unfused_optimizer">
<span id="deepspeed-pt-fp16-unfused-optimizer-module"></span><h2>deepspeed.pt.fp16_unfused_optimizer module<a class="headerlink" href="#module-deepspeed.pt.fp16_unfused_optimizer" title="Permalink to this headline">¶</a></h2>
<p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<p>Copyright NVIDIA/apex
This file is adapted from FP16_Optimizer in NVIDIA/apex</p>
<dl class="class">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.fp16_unfused_optimizer.</code><code class="sig-name descname">FP16_UnfusedOptimizer</code><span class="sig-paren">(</span><em class="sig-param">init_optimizer</em>, <em class="sig-param">static_loss_scale=1.0</em>, <em class="sig-param">dynamic_loss_scale=False</em>, <em class="sig-param">dynamic_loss_args=None</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">mpu=None</em>, <em class="sig-param">clip_grad=0.0</em>, <em class="sig-param">fused_lamb_legacy=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>FP16 Optimizer without weight fusion to support LAMB optimizer</p>
<p>For usage example please see, TODO:  DeepSpeed V2 Tutorial</p>
<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.backward" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.backward" title="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.backward"><code class="xref py py-attr docutils literal notranslate"><span class="pre">backward</span></code></a> performs the following steps:</p>
<ol class="arabic simple">
<li><p>fp32_loss = loss.float()</p></li>
<li><p>scaled_loss = fp32_loss*loss_scale</p></li>
<li><p>scaled_loss.backward(), which accumulates scaled gradients into the <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes of the model’s fp16 leaves</p></li>
</ol>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em>, <em class="sig-param">load_optimizer_states=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a state_dict created by an earlier call to state_dict().
If <code class="docutils literal notranslate"><span class="pre">fp16_optimizer_instance</span></code> was constructed from some <code class="docutils literal notranslate"><span class="pre">init_optimizer</span></code>,
whose parameters in turn came from <code class="docutils literal notranslate"><span class="pre">model</span></code>, it is expected that the user
will call <code class="docutils literal notranslate"><span class="pre">model.load_state_dict()</span></code> before
<code class="docutils literal notranslate"><span class="pre">fp16_optimizer_instance.load_state_dict()</span></code> is called.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">FP16_Optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">static_loss_scale</span> <span class="o">=</span> <span class="mf">128.0</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;saved.pth&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">])</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.param_groups">
<em class="property">property </em><code class="sig-name descname">param_groups</code><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.param_groups" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.state">
<em class="property">property </em><code class="sig-name descname">state</code><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dict containing the current state of this <code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code> instance.
This dict contains attributes of <code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code>, as well as the state_dict
of the contained Pytorch optimizer.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="s2">&quot;saved.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Not supporting closure.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.step_fused_lamb">
<code class="sig-name descname">step_fused_lamb</code><span class="sig-paren">(</span><em class="sig-param">closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.step_fused_lamb" title="Permalink to this definition">¶</a></dt>
<dd><p>Not supporting closure.</p>
</dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.unscale_and_clip_grads">
<code class="sig-name descname">unscale_and_clip_grads</code><span class="sig-paren">(</span><em class="sig-param">norm_groups</em>, <em class="sig-param">apply_scale=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.unscale_and_clip_grads" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><em class="sig-param">set_grads_to_None=True</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.fp16_unfused_optimizer.FP16_UnfusedOptimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Zero FP16 parameter grads.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-deepspeed.pt.loss_scaler">
<span id="deepspeed-pt-loss-scaler-module"></span><h2>deepspeed.pt.loss_scaler module<a class="headerlink" href="#module-deepspeed.pt.loss_scaler" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="deepspeed.pt.loss_scaler.DynamicLossScaler">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.loss_scaler.</code><code class="sig-name descname">DynamicLossScaler</code><span class="sig-paren">(</span><em class="sig-param">init_scale=4294967296</em>, <em class="sig-param">scale_factor=2.0</em>, <em class="sig-param">scale_window=1000</em>, <em class="sig-param">min_scale=1</em>, <em class="sig-param">delayed_shift=1</em>, <em class="sig-param">consecutive_hysteresis=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.DynamicLossScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class that manages dynamic loss scaling.  It is recommended to use <a class="reference internal" href="#deepspeed.pt.loss_scaler.DynamicLossScaler" title="deepspeed.pt.loss_scaler.DynamicLossScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">DynamicLossScaler</span></code></a>
indirectly, by supplying <code class="docutils literal notranslate"><span class="pre">dynamic_loss_scale=True</span></code> to the constructor of
<code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code>.  However, it’s important to understand how <a class="reference internal" href="#deepspeed.pt.loss_scaler.DynamicLossScaler" title="deepspeed.pt.loss_scaler.DynamicLossScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">DynamicLossScaler</span></code></a>
operates, because the default options can be changed using the
the <code class="docutils literal notranslate"><span class="pre">dynamic_loss_args</span></code> argument to <code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code>’s constructor.</p>
<p>Loss scaling is designed to combat the problem of underflowing gradients encountered at long
times when training fp16 networks.  Dynamic loss scaling begins by attempting a very high loss
scale.  Ironically, this may result in OVERflowing gradients.  If overflowing gradients are
encountered, <a class="reference internal" href="#deepspeed.pt.loss_scaler.DynamicLossScaler" title="deepspeed.pt.loss_scaler.DynamicLossScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">DynamicLossScaler</span></code></a> informs <code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code> that an overflow has
occurred.
<code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code> then skips the update step for this particular iteration/minibatch,
and <a class="reference internal" href="#deepspeed.pt.loss_scaler.DynamicLossScaler" title="deepspeed.pt.loss_scaler.DynamicLossScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">DynamicLossScaler</span></code></a> adjusts the loss scale to a lower value.
If a certain number of iterations occur without overflowing gradients detected,
<a class="reference internal" href="#deepspeed.pt.loss_scaler.DynamicLossScaler" title="deepspeed.pt.loss_scaler.DynamicLossScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">DynamicLossScaler</span></code></a> increases the loss scale once more.
In this way <a class="reference internal" href="#deepspeed.pt.loss_scaler.DynamicLossScaler" title="deepspeed.pt.loss_scaler.DynamicLossScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">DynamicLossScaler</span></code></a> attempts to “ride the edge” of
always using the highest loss scale possible without incurring overflow.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>init_scale</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=2**32</em>) – Initial loss scale attempted by <code class="xref py py-class docutils literal notranslate"><span class="pre">DynamicLossScaler.</span></code></p></li>
<li><p><strong>scale_factor</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=2.0</em>) – Factor used when adjusting the loss scale. If an overflow is encountered, the loss scale is readjusted to loss scale/<code class="docutils literal notranslate"><span class="pre">scale_factor</span></code>.  If <code class="docutils literal notranslate"><span class="pre">scale_window</span></code> consecutive iterations take place without an overflow, the loss scale is readjusted to loss_scale*``scale_factor``.</p></li>
<li><p><strong>scale_window</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1000</em>) – Number of consecutive iterations without an overflow to wait before increasing the loss scale.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="deepspeed.pt.loss_scaler.DynamicLossScaler.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">retain_graph=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.DynamicLossScaler.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.loss_scaler.DynamicLossScaler.has_overflow_serial">
<code class="sig-name descname">has_overflow_serial</code><span class="sig-paren">(</span><em class="sig-param">params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.DynamicLossScaler.has_overflow_serial" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.loss_scaler.DynamicLossScaler.loss_scale">
<em class="property">property </em><code class="sig-name descname">loss_scale</code><a class="headerlink" href="#deepspeed.pt.loss_scaler.DynamicLossScaler.loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.loss_scaler.DynamicLossScaler.scale_gradient">
<code class="sig-name descname">scale_gradient</code><span class="sig-paren">(</span><em class="sig-param">module</em>, <em class="sig-param">grad_in</em>, <em class="sig-param">grad_out</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.DynamicLossScaler.scale_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.loss_scaler.DynamicLossScaler.update_scale">
<code class="sig-name descname">update_scale</code><span class="sig-paren">(</span><em class="sig-param">overflow</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.DynamicLossScaler.update_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="deepspeed.pt.loss_scaler.LossScaler">
<em class="property">class </em><code class="sig-prename descclassname">deepspeed.pt.loss_scaler.</code><code class="sig-name descname">LossScaler</code><span class="sig-paren">(</span><em class="sig-param">scale=1</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.LossScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class that manages a static loss scale.  This class is intended to interact with
<code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code>, and should not be directly manipulated by the user.</p>
<p>Use of <a class="reference internal" href="#deepspeed.pt.loss_scaler.LossScaler" title="deepspeed.pt.loss_scaler.LossScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">LossScaler</span></code></a> is enabled via the <code class="docutils literal notranslate"><span class="pre">static_loss_scale</span></code> argument to
<code class="xref py py-class docutils literal notranslate"><span class="pre">FP16_Optimizer</span></code>’s constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scale</strong> (<em>float</em><em>, </em><em>optional</em><em>, </em><em>default=1.0</em>) – The loss scale.</p>
</dd>
</dl>
<dl class="method">
<dt id="deepspeed.pt.loss_scaler.LossScaler.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">retain_graph=False</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.LossScaler.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.loss_scaler.LossScaler.has_overflow">
<code class="sig-name descname">has_overflow</code><span class="sig-paren">(</span><em class="sig-param">params</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.LossScaler.has_overflow" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.loss_scaler.LossScaler.loss_scale">
<em class="property">property </em><code class="sig-name descname">loss_scale</code><a class="headerlink" href="#deepspeed.pt.loss_scaler.LossScaler.loss_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.loss_scaler.LossScaler.scale_gradient">
<code class="sig-name descname">scale_gradient</code><span class="sig-paren">(</span><em class="sig-param">module</em>, <em class="sig-param">grad_in</em>, <em class="sig-param">grad_out</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.LossScaler.scale_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="deepspeed.pt.loss_scaler.LossScaler.update_scale">
<code class="sig-name descname">update_scale</code><span class="sig-paren">(</span><em class="sig-param">overflow</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.LossScaler.update_scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="deepspeed.pt.loss_scaler.to_python_float">
<code class="sig-prename descclassname">deepspeed.pt.loss_scaler.</code><code class="sig-name descname">to_python_float</code><span class="sig-paren">(</span><em class="sig-param">t</em><span class="sig-paren">)</span><a class="headerlink" href="#deepspeed.pt.loss_scaler.to_python_float" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-deepspeed.pt">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-deepspeed.pt" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="deepspeed.html" class="btn btn-neutral float-left" title="deepspeed package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Microsoft AI &amp; Research

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>